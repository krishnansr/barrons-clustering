{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from adjustText import adjust_text\n",
    "import operator\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import word2vec\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.manifold import TSNE\n",
    "# from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook hparams\n",
    "corpus_path = r\"..\\barrons_333_corpus.txt\"\n",
    "W2V_SIZE = 100    # Word vector dimensionality\n",
    "W2V_WINDOW = 30   # Context window size\n",
    "W2V_MIN_COUNT = 1    # Minimum word count\n",
    "W2V_EPOCHS = 50    # w2v model training iters\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting gensim pretrained model & corpora info\n",
    "def get_gensim_pretrained_info(entity, desc_len=None):\n",
    "    \"\"\"\n",
    "    :param entity: either 'corpora' or 'models'\n",
    "    :param desc_len: description length of each entity, entire description is printed if this is None\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    info = api.info()\n",
    "    for entity_name, entity_data in sorted(info[entity].items()):\n",
    "        print(f\"{entity_name:<40} {entity_data.get('num_records', -1)} records: \"\n",
    "              f\"{entity_data['description'][:desc_len] + '...'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize a corpus using nltk\n",
    "def nltk_corpus_tokenizer(corpus):\n",
    "    # tokenize sentences in corpus\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    tokenized_corpus = wpt.tokenize(corpus)\n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a w2v model on a given corpus\n",
    "def train_w2v_model(corpus, size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, iters=W2V_EPOCHS, workers=4):\n",
    "    logging.info(f'word2vec model training started with params {size, window, min_count, iters, workers}')\n",
    "    w2v_model = word2vec.Word2Vec(corpus,\n",
    "                                  size=size,\n",
    "                                  window=window,\n",
    "                                  min_count=min_count,\n",
    "                                  iter=iters,\n",
    "                                  workers=workers)\n",
    "    logging.info(f'word2vec model training completed..')\n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracting w2v feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of raw corpus: 334\n",
      "Wall time: 998 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(corpus_path, 'r') as f:\n",
    "    barrons_corpus = f.read()\n",
    "    \n",
    "tokenized_corpus = nltk_corpus_tokenizer(barrons_corpus)\n",
    "print(f'length of raw corpus: {len(tokenized_corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_corpus_vocab(w2v_model, tokenized_corpus):\n",
    "    trained_words = list()\n",
    "    untrained_words = list()\n",
    "    model_vocab = w2v_model.vocab\n",
    "    for _word in tokenized_corpus:\n",
    "        if _word in model_vocab:\n",
    "            trained_words.append(_word)\n",
    "        else:\n",
    "            untrained_words.append(_word)\n",
    "    \n",
    "    print(f\"w2v model doesn't have {len(untrained_words)} words: {untrained_words}\")\n",
    "    return trained_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__testing_word2vec-matrix-synopsis       -1 records: [THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix....\n",
      "conceptnet-numberbatch-17-06-300         1917247 records: ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known...\n",
      "fasttext-wiki-news-subwords-300          999999 records: 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt...\n",
      "glove-twitter-100                        1193514 records: Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https:...\n",
      "glove-twitter-200                        1193514 records: Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https:/...\n",
      "glove-twitter-25                         1193514 records: Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https:/...\n",
      "glove-twitter-50                         1193514 records: Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https:/...\n",
      "glove-wiki-gigaword-100                  400000 records: Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, ...\n",
      "glove-wiki-gigaword-200                  400000 records: Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab,...\n",
      "glove-wiki-gigaword-300                  400000 records: Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab,...\n",
      "glove-wiki-gigaword-50                   400000 records: Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab,...\n",
      "word2vec-google-news-300                 3000000 records: Pre-trained vectors trained on a part of the Google News dataset (about 100 bill...\n",
      "word2vec-ruscorpora-300                  184973 records: Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (ab...\n"
     ]
    }
   ],
   "source": [
    "get_gensim_pretrained_info('models', desc_len=80)\n",
    "# w2v_model = train_w2v_model(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v model doesn't have 3 words: ['desiccate', 'enervate', 'veracious']\n",
      "length of filtered corpus: 331\n",
      "Wall time: 32.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load pre trained model\n",
    "w2v_model_type = \"glove-wiki-gigaword-50\"\n",
    "# w2v_model_type = \"glove-twitter-100\"  # doesn't work well\n",
    "# w2v_model_type = \"glove-wiki-gigaword-300\"\n",
    "\n",
    "w2v_model = api.load(w2v_model_type)\n",
    "tokenized_corpus = filter_corpus_vocab(w2v_model, tokenized_corpus)\n",
    "print(f'length of filtered corpus: {len(tokenized_corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('stopping', 0.8896316885948181), ('stopped', 0.8306300640106201), ('trying', 0.8275439739227295), ('tried', 0.8172997832298279), ('stops', 0.8129743933677673)]\n",
      "[('girl', 0.9065280556678772), ('man', 0.8860336542129517), ('mother', 0.8763703107833862), ('her', 0.86131352186203), ('boy', 0.8596119284629822)]\n",
      "[('woman', 0.8860337734222412), ('boy', 0.8564431071281433), ('another', 0.8452839851379395), ('old', 0.8372182846069336), ('one', 0.8276063203811646)]\n",
      "[('caputo', 0.6675522923469543), ('washpost.com', 0.6115913391113281), ('levesque', 0.6013662815093994), ('subside', 0.5964425206184387), ('greef', 0.5947305560112)]\n",
      "[('indian', 0.8648794889450073), ('pakistan', 0.8529723286628723), ('malaysia', 0.816650927066803), ('bangladesh', 0.8154239058494568), ('delhi', 0.8142766952514648)]\n"
     ]
    }
   ],
   "source": [
    "# sample test runs\n",
    "for _word in [\"stop\", \"woman\", \"man\", \"abate\", \"india\"]:\n",
    "    neighbors = w2v_model.most_similar(_word, topn=5)\n",
    "    print(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(331, 50)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get word vectors for all corpus words\n",
    "wvs = w2v_model.wv[tokenized_corpus]\n",
    "wvs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build feature array for all tokens in corpus to cluster\n",
    "w2v_feature_array = wvs.copy()  # if using barrons word meanings, average feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBScan clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering metrics {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, -1}: 17, \n",
      "84 noise clusters out of 331\n",
      "[ 0 -1  1  0  0  0  0  0 -1  2  0  3  4  0  0 -1  0  0  0  0  0  0  0  5\n",
      "  0  0  0  0  0  0  0  0  0 -1 -1  0 -1  0  0  0  0 -1  0  0 -1 -1  0  6\n",
      "  0 -1  0  7  0  0 -1 -1  0  0  0  0 -1  3 -1  0  0  0  0 -1  0  0 -1  5\n",
      "  0  0  0  0  0  0  0  0  0  0  8  7  0  0 -1  0  0  0  0  0  0  0  0  3\n",
      "  0 -1 -1 -1  0 -1  0 -1  0 -1  1  0  0  0  9 -1  7 -1 -1  9  3 10  0  0\n",
      " 11  2  0  0  0  7 -1  0  0 -1  9  0  0  0 -1  0  0  0 -1  0  0  0  0  0\n",
      "  0  0  0  0 -1  0  0  0  0  0  0 -1  0 -1  0  0 -1  0 12  0 12 -1 13 -1\n",
      " -1 -1  0  0  3 -1  0 -1  0  0  0  0 -1  0  0  0  0  0 14 -1  0  0 -1 -1\n",
      "  0 -1  0 -1  0 -1 -1 -1  0 -1 -1  2  0 -1  0  0  7  0 -1  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0 -1  0 -1 -1  0 -1  0 -1  0  0 -1 10 -1  0 12 -1  0\n",
      "  0  0 -1  8  3 -1 12 -1  0 15  0  6  0  0 11  0  4  0  0 -1 -1 -1  0  0\n",
      "  0  0  3  0 -1 -1  0  0  0 -1 12  0  3 -1  0  0  0  0  0  0  0 -1 16  0\n",
      "  0 14 -1  0 -1  0  0 16  0 -1 -1  0 -1  0  0 -1 -1  0  0  0  0  0 13  0\n",
      " 15  0 -1  0 -1  0  0 -1  0  0  0  4  0  0 -1  0 -1  0  0]\n",
      "Wall time: 5.95 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build dbscan clustering model\n",
    "# dbscan_model = DBSCAN(metric='cosine', eps=0.5, min_samples=3)\n",
    "dbscan_model = DBSCAN(metric='cosine', eps=0.5, min_samples=2, algorithm=\"auto\")\n",
    "\n",
    "w2v_feature_array = StandardScaler().fit_transform(w2v_feature_array)\n",
    "clustering = dbscan_model.fit(w2v_feature_array)\n",
    "labels = clustering.labels_\n",
    "core_samples = clustering.core_sample_indices_\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = len([lab for lab in labels if lab == -1])\n",
    "\n",
    "print(f\"clustering metrics {set(labels)}: {n_clusters}, \\n{n_noise} noise clusters out of {len(labels)}\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the corpus is segregated into 17 clusters\n"
     ]
    }
   ],
   "source": [
    "corpus_clusters_df = pd.DataFrame({'words': tokenized_corpus, 'cluter_labels': labels})\n",
    "print(f\"the corpus is segregated into {n_clusters} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>cluter_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aberrant</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abeyance</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abscond</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abstemious</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>volatile</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>wary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>welter</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>whimsical</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>zealot</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>331 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          words  cluter_labels\n",
       "0         abate              0\n",
       "1      aberrant             -1\n",
       "2      abeyance              1\n",
       "3       abscond              0\n",
       "4    abstemious              0\n",
       "..          ...            ...\n",
       "326    volatile             -1\n",
       "327        wary              0\n",
       "328      welter             -1\n",
       "329   whimsical              0\n",
       "330      zealot              0\n",
       "\n",
       "[331 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_clusters_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save clusters to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "conv_clusters_df = pd.DataFrame()\n",
    "\n",
    "count = 0\n",
    "for cluster_id, subset_df in corpus_clusters_df.sort_values(by=\"cluter_labels\").groupby(\"cluter_labels\"):\n",
    "    conv_clusters_df = pd.concat([conv_clusters_df, subset_df.words.reset_index(drop=True)], ignore_index=True, axis=1)\n",
    "\n",
    "conv_clusters_df.fillna(value='', inplace=True)\n",
    "conv_clusters_df.to_csv(f\"../res/{w2v_model_type}-dbs_clusters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>implacable</td>\n",
       "      <td>loquacious</td>\n",
       "      <td>abeyance</td>\n",
       "      <td>alacrity</td>\n",
       "      <td>plethora</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>approbation</td>\n",
       "      <td>precursor</td>\n",
       "      <td>metamorphosis</td>\n",
       "      <td>digression</td>\n",
       "      <td>extrapolation</td>\n",
       "      <td>perennial</td>\n",
       "      <td>pristine</td>\n",
       "      <td>impermeable</td>\n",
       "      <td>implicit</td>\n",
       "      <td>saturate</td>\n",
       "      <td>precarious</td>\n",
       "      <td>salubrious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ostentatious</td>\n",
       "      <td>tirade</td>\n",
       "      <td>dormant</td>\n",
       "      <td>equanimity</td>\n",
       "      <td>disparate</td>\n",
       "      <td>viable</td>\n",
       "      <td>deference</td>\n",
       "      <td>catalyst</td>\n",
       "      <td>eulogy</td>\n",
       "      <td>platitude</td>\n",
       "      <td>empirical</td>\n",
       "      <td>endemic</td>\n",
       "      <td>ephemeral</td>\n",
       "      <td>porous</td>\n",
       "      <td>tacit</td>\n",
       "      <td>inundate</td>\n",
       "      <td>tenuous</td>\n",
       "      <td>soporific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paragon</td>\n",
       "      <td>mollify</td>\n",
       "      <td></td>\n",
       "      <td>magnanimity</td>\n",
       "      <td>emulate</td>\n",
       "      <td>problematic</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>elegy</td>\n",
       "      <td></td>\n",
       "      <td>efficacy</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>impervious</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pate</td>\n",
       "      <td>tangential</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>relegate</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>coda</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>permeable</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dogmatic</td>\n",
       "      <td>substantiate</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>contention</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>dirge</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>refractory</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td></td>\n",
       "      <td>enhance</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td></td>\n",
       "      <td>dupe</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td></td>\n",
       "      <td>ebullient</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td></td>\n",
       "      <td>document</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td></td>\n",
       "      <td>eclectic</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0             1         2            3           4   \\\n",
       "0      implacable    loquacious  abeyance     alacrity    plethora   \n",
       "1    ostentatious        tirade   dormant   equanimity   disparate   \n",
       "2         paragon       mollify            magnanimity     emulate   \n",
       "3            pate    tangential                           relegate   \n",
       "4        dogmatic  substantiate                         contention   \n",
       "..            ...           ...       ...          ...         ...   \n",
       "195                     enhance                                      \n",
       "196                        dupe                                      \n",
       "197                   ebullient                                      \n",
       "198                    document                                      \n",
       "199                    eclectic                                      \n",
       "\n",
       "              5            6          7              8           9   \\\n",
       "0      ambiguous  approbation  precursor  metamorphosis  digression   \n",
       "1         viable    deference   catalyst         eulogy   platitude   \n",
       "2    problematic                                  elegy               \n",
       "3                                                  coda               \n",
       "4                                                 dirge               \n",
       "..           ...          ...        ...            ...         ...   \n",
       "195                                                                   \n",
       "196                                                                   \n",
       "197                                                                   \n",
       "198                                                                   \n",
       "199                                                                   \n",
       "\n",
       "                10         11         12           13        14        15  \\\n",
       "0    extrapolation  perennial   pristine  impermeable  implicit  saturate   \n",
       "1        empirical    endemic  ephemeral       porous     tacit  inundate   \n",
       "2         efficacy                         impervious                       \n",
       "3                                           permeable                       \n",
       "4                                          refractory                       \n",
       "..             ...        ...        ...          ...       ...       ...   \n",
       "195                                                                         \n",
       "196                                                                         \n",
       "197                                                                         \n",
       "198                                                                         \n",
       "199                                                                         \n",
       "\n",
       "             16          17  \n",
       "0    precarious  salubrious  \n",
       "1       tenuous   soporific  \n",
       "2                            \n",
       "3                            \n",
       "4                            \n",
       "..          ...         ...  \n",
       "195                          \n",
       "196                          \n",
       "197                          \n",
       "198                          \n",
       "199                          \n",
       "\n",
       "[200 rows x 18 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_clusters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
