{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from adjustText import adjust_text\n",
    "import operator\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import word2vec\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.manifold import TSNE\n",
    "# from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook hparams\n",
    "corpus_path = r\"..\\barrons_333_corpus.txt\"\n",
    "W2V_SIZE = 100    # Word vector dimensionality\n",
    "W2V_WINDOW = 30   # Context window size\n",
    "W2V_MIN_COUNT = 1    # Minimum word count\n",
    "W2V_EPOCHS = 50    # w2v model training iters\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting gensim pretrained model & corpora info\n",
    "def get_gensim_pretrained_info(entity, desc_len=None):\n",
    "    \"\"\"\n",
    "    :param entity: either 'corpora' or 'models'\n",
    "    :param desc_len: description length of each entity, entire description is printed if this is None\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    info = api.info()\n",
    "    for entity_name, entity_data in sorted(info[entity].items()):\n",
    "        print(f\"{entity_name:<40} {entity_data.get('num_records', -1)} records: \"\n",
    "              f\"{entity_data['description'][:desc_len] + '...'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize a corpus using nltk\n",
    "def nltk_corpus_tokenizer(corpus):\n",
    "    # tokenize sentences in corpus\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    tokenized_corpus = wpt.tokenize(corpus)\n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a w2v model on a given corpus\n",
    "def train_w2v_model(corpus, size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, iters=W2V_EPOCHS, workers=4):\n",
    "    logging.info(f'word2vec model training started with params {size, window, min_count, iters, workers}')\n",
    "    w2v_model = word2vec.Word2Vec(corpus,\n",
    "                                  size=size,\n",
    "                                  window=window,\n",
    "                                  min_count=min_count,\n",
    "                                  iter=iters,\n",
    "                                  workers=workers)\n",
    "    logging.info(f'word2vec model training completed..')\n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracting w2v feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of raw corpus: 334\n",
      "Wall time: 1.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(corpus_path, 'r') as f:\n",
    "    barrons_corpus = f.read()\n",
    "    \n",
    "tokenized_corpus = nltk_corpus_tokenizer(barrons_corpus)\n",
    "print(f'length of raw corpus: {len(tokenized_corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_corpus_vocab(w2v_model, tokenized_corpus):\n",
    "    trained_words = list()\n",
    "    untrained_words = list()\n",
    "    model_vocab = w2v_model.vocab\n",
    "    for _word in tokenized_corpus:\n",
    "        if _word in model_vocab:\n",
    "            trained_words.append(_word)\n",
    "        else:\n",
    "            untrained_words.append(_word)\n",
    "    \n",
    "    print(f\"w2v model doesn't have {len(untrained_words)} words: {untrained_words}\")\n",
    "    return trained_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__testing_word2vec-matrix-synopsis       -1 records: [THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix....\n",
      "conceptnet-numberbatch-17-06-300         1917247 records: ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known...\n",
      "fasttext-wiki-news-subwords-300          999999 records: 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt...\n",
      "glove-twitter-100                        1193514 records: Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https:...\n",
      "glove-twitter-200                        1193514 records: Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https:/...\n",
      "glove-twitter-25                         1193514 records: Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https:/...\n",
      "glove-twitter-50                         1193514 records: Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https:/...\n",
      "glove-wiki-gigaword-100                  400000 records: Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, ...\n",
      "glove-wiki-gigaword-200                  400000 records: Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab,...\n",
      "glove-wiki-gigaword-300                  400000 records: Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab,...\n",
      "glove-wiki-gigaword-50                   400000 records: Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab,...\n",
      "word2vec-google-news-300                 3000000 records: Pre-trained vectors trained on a part of the Google News dataset (about 100 bill...\n",
      "word2vec-ruscorpora-300                  184973 records: Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (ab...\n"
     ]
    }
   ],
   "source": [
    "get_gensim_pretrained_info('models', desc_len=80)\n",
    "# w2v_model = train_w2v_model(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v model doesn't have 3 words: ['desiccate', 'enervate', 'veracious']\n",
      "length of filtered corpus: 331\n",
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load pre trained model\n",
    "# w2v_model_type = \"glove-wiki-gigaword-50\"\n",
    "# w2v_model_type = \"glove-twitter-100\"  # doesn't work well\n",
    "w2v_model_type = \"glove-wiki-gigaword-300\"\n",
    "\n",
    "w2v_model = api.load(w2v_model_type)\n",
    "tokenized_corpus = filter_corpus_vocab(w2v_model, tokenized_corpus)\n",
    "print(f'length of filtered corpus: {len(tokenized_corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('stopping', 0.758859395980835), ('stops', 0.7051315903663635), ('stopped', 0.6975031495094299), ('halt', 0.6571016311645508), ('prevent', 0.6046357750892639)]\n",
      "[('girl', 0.7296419143676758), ('man', 0.6998662948608398), ('mother', 0.689943790435791), ('she', 0.6433226466178894), ('her', 0.6327143311500549)]\n",
      "[('woman', 0.6998662948608398), ('person', 0.6443442106246948), ('boy', 0.620827853679657), ('he', 0.5926738977432251), ('men', 0.5819568634033203)]\n",
      "[('subside', 0.4828464388847351), ('abated', 0.42604950070381165), ('ignazio', 0.39123255014419556), ('slacken', 0.38261228799819946), ('falter', 0.37811487913131714)]\n",
      "[('indian', 0.7355823516845703), ('pakistan', 0.7285579442977905), ('delhi', 0.6846905946731567), ('bangladesh', 0.620319128036499), ('lanka', 0.609517514705658)]\n"
     ]
    }
   ],
   "source": [
    "# sample test runs\n",
    "for _word in [\"stop\", \"woman\", \"man\", \"abate\", \"india\"]:\n",
    "    neighbors = w2v_model.most_similar(_word, topn=5)\n",
    "    print(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(331, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get word vectors for all corpus words\n",
    "wvs = w2v_model.wv[tokenized_corpus]\n",
    "wvs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build feature array for all tokens in corpus to cluster\n",
    "w2v_feature_array = wvs.copy()  # if using barrons word meanings, average feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBScan clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering metrics {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, -1}: 15, \n",
      "294 noise clusters out of 331\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1  0 -1 -1 -1 -1 -1 -1  1 -1 -1\n",
      " -1  2 -1 -1 -1  1 -1 -1 -1 -1 -1  3 -1 -1  4 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  5 -1 -1 -1 -1 -1 -1\n",
      " -1  6 -1 -1 -1 -1  7 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  6 -1\n",
      " -1 -1 -1 -1 -1 -1  5 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  4\n",
      " -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1  4  8 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1  9 -1 -1 -1 -1 -1 -1  7 -1 -1 -1 -1 -1 10 -1 10 11 -1 -1 -1 12 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  7 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1  9 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0  1 -1  3 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 11 -1 -1\n",
      " -1  1 -1 -1 -1 -1 -1 -1 -1 13 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 14 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1  8 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 14 -1 -1 12 -1\n",
      " 13  7 -1  2 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "Wall time: 9.97 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build dbscan clustering model\n",
    "# dbscan_model = DBSCAN(metric='cosine', eps=0.5, min_samples=3)\n",
    "dbscan_model = DBSCAN(metric='cosine', eps=0.5, min_samples=2, algorithm=\"auto\")\n",
    "\n",
    "w2v_feature_array = StandardScaler().fit_transform(w2v_feature_array)\n",
    "clustering = dbscan_model.fit(w2v_feature_array)\n",
    "labels = clustering.labels_\n",
    "core_samples = clustering.core_sample_indices_\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = len([lab for lab in labels if lab == -1])\n",
    "\n",
    "print(f\"clustering metrics {set(labels)}: {n_clusters}, \\n{n_noise} noise clusters out of {len(labels)}\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the corpus is segregated into 15 clusters\n"
     ]
    }
   ],
   "source": [
    "corpus_clusters_df = pd.DataFrame({'words': tokenized_corpus, 'cluter_labels': labels})\n",
    "print(f\"the corpus is segregated into {n_clusters} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>cluter_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abate</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aberrant</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abeyance</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abscond</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abstemious</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>volatile</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>wary</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>welter</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>whimsical</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>zealot</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>331 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          words  cluter_labels\n",
       "0         abate             -1\n",
       "1      aberrant             -1\n",
       "2      abeyance             -1\n",
       "3       abscond             -1\n",
       "4    abstemious             -1\n",
       "..          ...            ...\n",
       "326    volatile             -1\n",
       "327        wary             -1\n",
       "328      welter             -1\n",
       "329   whimsical             -1\n",
       "330      zealot             -1\n",
       "\n",
       "[331 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_clusters_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save clusters to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "conv_clusters_df = pd.DataFrame()\n",
    "\n",
    "count = 0\n",
    "for cluster_id, subset_df in corpus_clusters_df.sort_values(by=\"cluter_labels\").groupby(\"cluter_labels\"):\n",
    "    conv_clusters_df = pd.concat([conv_clusters_df, subset_df.words.reset_index(drop=True)], ignore_index=True, axis=1)\n",
    "\n",
    "conv_clusters_df.fillna(value='', inplace=True)\n",
    "conv_clusters_df.to_csv(f\"../res/{w2v_model_type}-dbs_clusters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abate</td>\n",
       "      <td>alleviate</td>\n",
       "      <td>placate</td>\n",
       "      <td>tortuous</td>\n",
       "      <td>banal</td>\n",
       "      <td>facilitate</td>\n",
       "      <td>converge</td>\n",
       "      <td>denigrate</td>\n",
       "      <td>harangue</td>\n",
       "      <td>specious</td>\n",
       "      <td>garrulous</td>\n",
       "      <td>impede</td>\n",
       "      <td>permeable</td>\n",
       "      <td>tacit</td>\n",
       "      <td>tenuous</td>\n",
       "      <td>substantiate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>occlude</td>\n",
       "      <td>ameliorate</td>\n",
       "      <td>appease</td>\n",
       "      <td>arduous</td>\n",
       "      <td>mundane</td>\n",
       "      <td>enhance</td>\n",
       "      <td>diverge</td>\n",
       "      <td>disparage</td>\n",
       "      <td>diatribe</td>\n",
       "      <td>fallacious</td>\n",
       "      <td>loquacious</td>\n",
       "      <td>impair</td>\n",
       "      <td>impermeable</td>\n",
       "      <td>implicit</td>\n",
       "      <td>precarious</td>\n",
       "      <td>refute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obviate</td>\n",
       "      <td>exacerbate</td>\n",
       "      <td>assuage</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>bolster</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>invective</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>obsequious</td>\n",
       "      <td>mitigate</td>\n",
       "      <td>mollify</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>tirade</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>obdurate</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>efficacy</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>effrontery</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>disjointed</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>disparate</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>distill</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>294 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0           1        2         3        4           5         6   \\\n",
       "0         abate   alleviate  placate  tortuous    banal  facilitate  converge   \n",
       "1       occlude  ameliorate  appease   arduous  mundane     enhance   diverge   \n",
       "2       obviate  exacerbate  assuage                        bolster             \n",
       "3    obsequious    mitigate  mollify                                            \n",
       "4      obdurate                                                                 \n",
       "..          ...         ...      ...       ...      ...         ...       ...   \n",
       "289    efficacy                                                                 \n",
       "290  effrontery                                                                 \n",
       "291  disjointed                                                                 \n",
       "292   disparate                                                                 \n",
       "293     distill                                                                 \n",
       "\n",
       "            7          8           9           10      11           12  \\\n",
       "0    denigrate   harangue    specious   garrulous  impede    permeable   \n",
       "1    disparage   diatribe  fallacious  loquacious  impair  impermeable   \n",
       "2               invective                                                \n",
       "3                  tirade                                                \n",
       "4                                                                        \n",
       "..         ...        ...         ...         ...     ...          ...   \n",
       "289                                                                      \n",
       "290                                                                      \n",
       "291                                                                      \n",
       "292                                                                      \n",
       "293                                                                      \n",
       "\n",
       "           13          14            15  \n",
       "0       tacit     tenuous  substantiate  \n",
       "1    implicit  precarious        refute  \n",
       "2                                        \n",
       "3                                        \n",
       "4                                        \n",
       "..        ...         ...           ...  \n",
       "289                                      \n",
       "290                                      \n",
       "291                                      \n",
       "292                                      \n",
       "293                                      \n",
       "\n",
       "[294 rows x 16 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_clusters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
