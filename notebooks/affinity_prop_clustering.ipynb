{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import word2vec\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook hparams\n",
    "corpus_path = r\"..\\barrons_333_corpus.txt\"\n",
    "W2V_SIZE = 100    # Word vector dimensionality\n",
    "W2V_WINDOW = 30   # Context window size\n",
    "W2V_MIN_COUNT = 1    # Minimum word count\n",
    "W2V_EPOCHS = 50    # w2v model training iters\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting gensim pretrained model & corpora info\n",
    "def get_gensim_pretrained_info(entity, desc_len=None):\n",
    "    \"\"\"\n",
    "    :param entity: either 'corpora' or 'models'\n",
    "    :param desc_len: description length of each entity, entire description is printed if this is None\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    info = api.info()\n",
    "    for entity_name, entity_data in sorted(info[entity].items()):\n",
    "        print(f\"{entity_name:<40} {entity_data.get('num_records', -1)} records: \"\n",
    "              f\"{entity_data['description'][:desc_len] + '...'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize a corpus using nltk\n",
    "def nltk_corpus_tokenizer(corpus):\n",
    "    # tokenize sentences in corpus\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    tokenized_corpus = wpt.tokenize(corpus)\n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a w2v model on a given corpus\n",
    "def train_w2v_model(corpus, size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, iter=W2V_EPOCHS, workers=4):\n",
    "    logging.info(f'word2vec model training started with params {size, window, min_count, iter, workers}')\n",
    "    w2v_model = word2vec.Word2Vec(corpus,\n",
    "                                  size=size,\n",
    "                                  window=window,\n",
    "                                  min_count=min_count,\n",
    "                                  iter=iter,\n",
    "                                  workers=workers)\n",
    "    logging.info(f'word2vec model training completed..')\n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting w2v vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__testing_word2vec-matrix-synopsis       -1 records: [THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix....\n",
      "conceptnet-numberbatch-17-06-300         1917247 records: ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known...\n",
      "fasttext-wiki-news-subwords-300          999999 records: 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt...\n",
      "glove-twitter-100                        1193514 records: Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https:...\n",
      "glove-twitter-200                        1193514 records: Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https:/...\n",
      "glove-twitter-25                         1193514 records: Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https:/...\n",
      "glove-twitter-50                         1193514 records: Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https:/...\n",
      "glove-wiki-gigaword-100                  400000 records: Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, ...\n",
      "glove-wiki-gigaword-200                  400000 records: Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab,...\n",
      "glove-wiki-gigaword-300                  400000 records: Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab,...\n",
      "glove-wiki-gigaword-50                   400000 records: Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab,...\n",
      "word2vec-google-news-300                 3000000 records: Pre-trained vectors trained on a part of the Google News dataset (about 100 bill...\n",
      "word2vec-ruscorpora-300                  184973 records: Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (ab...\n"
     ]
    }
   ],
   "source": [
    "get_gensim_pretrained_info('models', desc_len=80)\n",
    "# w2v_model = train_w2v_model(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n"
     ]
    }
   ],
   "source": [
    "with open(corpus_path, 'r') as f:\n",
    "    barrons_corpus = f.read()\n",
    "    \n",
    "tokenized_corpus = nltk_corpus_tokenizer(barrons_corpus)\n",
    "print(f'length of raw corpus {len(tokenized_corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_corpus_vocab(w2v_model, tokenized_corpus):\n",
    "    trained_words = list()\n",
    "    untrained_words = list()\n",
    "    model_vocab = w2v_model.vocab\n",
    "    for _word in tokenized_corpus:\n",
    "        if _word in model_vocab:\n",
    "            trained_words.append(_word)\n",
    "        else:\n",
    "            untrained_words.append(_word)\n",
    "    \n",
    "    print(f\"w2v model doesn't have {len(untrained_words)} words: {untrained_words}\")\n",
    "    return trained_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre trained model\n",
    "w2v_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "tokenized_corpus = _filter_corpus_vocab(w2v_model, tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('stopping', 0.8896316885948181), ('stopped', 0.8306300640106201), ('trying', 0.8275439739227295), ('tried', 0.8172997832298279), ('stops', 0.8129743933677673)]\n",
      "[('girl', 0.9065280556678772), ('man', 0.8860336542129517), ('mother', 0.8763703107833862), ('her', 0.86131352186203), ('boy', 0.8596119284629822)]\n",
      "[('woman', 0.8860337734222412), ('boy', 0.8564431071281433), ('another', 0.8452839851379395), ('old', 0.8372182846069336), ('one', 0.8276063203811646)]\n",
      "[('caputo', 0.6675522923469543), ('washpost.com', 0.6115913391113281), ('levesque', 0.6013662815093994), ('subside', 0.5964425206184387), ('greef', 0.5947305560112)]\n",
      "[('indian', 0.8648794889450073), ('pakistan', 0.8529723286628723), ('malaysia', 0.816650927066803), ('bangladesh', 0.8154239058494568), ('delhi', 0.8142766952514648)]\n"
     ]
    }
   ],
   "source": [
    "# sample runs\n",
    "for _word in [\"stop\", \"woman\", \"man\", \"abate\", \"india\"]:\n",
    "    neighbors = w2v_model.most_similar(_word, topn=5)\n",
    "    print(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs = w2v_model.wv[tokenized_corpus]\n",
    "print(len(wvs), wvs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 True False\n"
     ]
    }
   ],
   "source": [
    "q = w2v_model.vocab.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=2)\n",
    "T = tsne.fit_transform(wvs)\n",
    "\n",
    "labels = tokenized_corpus\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
